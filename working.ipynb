{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import wandb\n",
    "import torch\n",
    "import logging\n",
    "from arguments import parser\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from train import refinement_run\n",
    "from datasets import create_dataset\n",
    "from log import setup_default_logging\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from torch.distributed import get_rank\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "default_setting = './configs/benchmark/default_setting.yaml'\n",
    "cfg = parser(jupyter=True, default_setting = default_setting)\n",
    "\n",
    "\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision             = cfg.TRAIN.mixed_precision\n",
    ")\n",
    "\n",
    "# load dataset\n",
    "trainset, validset, testset = create_dataset(\n",
    "    dataset_name  = cfg.DATASET.dataset_name,\n",
    "    datadir       = cfg.DATASET.datadir,\n",
    "    class_name    = cfg.DATASET.class_name,\n",
    "    img_size      = cfg.DATASET.img_size,\n",
    "    mean          = cfg.DATASET.mean,\n",
    "    std           = cfg.DATASET.std,\n",
    "    aug_info      = cfg.DATASET.aug_info,\n",
    "    anomaly_ratio = 0.1,\n",
    "    **cfg.DATASET.get('params',{})\n",
    ")\n",
    "\n",
    "# make save directory\n",
    "savedir = os.path.join(\n",
    "                            cfg.DEFAULT.savedir,\n",
    "                            cfg.DATASET.dataset_name,\n",
    "                            cfg.DATASET.class_name\n",
    "                        )\n",
    "\n",
    "exp_name         = cfg.DEFAULT.exp_name\n",
    "\n",
    "method            = cfg.MODEL.method\n",
    "model_name        = cfg.MODEL.model_name\n",
    "model_params      = cfg.MODEL.get('params',{})\n",
    "\n",
    "trainset         = trainset\n",
    "validset         = validset\n",
    "testset          = testset\n",
    "\n",
    "batch_size       = cfg.DATASET.batch_size\n",
    "test_batch_size  = cfg.DATASET.test_batch_size\n",
    "num_workers      = cfg.DATASET.num_workers\n",
    "\n",
    "opt_name         = cfg.OPTIMIZER.opt_name\n",
    "lr               = cfg.OPTIMIZER.lr\n",
    "opt_params       = cfg.OPTIMIZER.get('params',{})\n",
    "\n",
    "epochs           = cfg.TRAIN.epochs\n",
    "log_interval     = cfg.TRAIN.log_interval\n",
    "use_wandb        = cfg.TRAIN.wandb.use\n",
    "\n",
    "savedir          = savedir\n",
    "seed             = cfg.DEFAULT.seed\n",
    "accelerator      = accelerator\n",
    "cfg              = cfg\n",
    "\n",
    "model = __import__('models').__dict__[method](\n",
    "    model_name = model_name,\n",
    "    **model_params\n",
    "    )    \n",
    "\n",
    "# define train dataloader\n",
    "trainloader = DataLoader(\n",
    "    dataset     = trainset,\n",
    "    batch_size  = batch_size,\n",
    "    num_workers = num_workers\n",
    ")\n",
    "\n",
    "validloader = DataLoader(\n",
    "    dataset     = validset,\n",
    "    batch_size  = test_batch_size,\n",
    "    num_workers = num_workers\n",
    ")\n",
    "\n",
    "# define test dataloader\n",
    "testloader = DataLoader(\n",
    "    dataset     = testset,\n",
    "    batch_size  = test_batch_size,\n",
    "    shuffle     = False,\n",
    "    num_workers = num_workers\n",
    ")\n",
    "\n",
    "optimizer = __import__('torch.optim', fromlist='optim').__dict__[opt_name](model.parameters(), lr=lr, **opt_params)\n",
    "\n",
    "scheduler = None \n",
    "\n",
    "model, optimizer, trainloader, testloader, scheduler = accelerator.prepare(\n",
    "            model, optimizer, trainloader, testloader, scheduler\n",
    "        )\n",
    "\n",
    "r = 0 \n",
    "\n",
    "from sklearn.metrics import roc_auc_score,auc \n",
    "from scipy.ndimage import gaussian_filter\n",
    "import numpy as np \n",
    "from numpy import ndarray\n",
    "import pandas as pd \n",
    "import torch.nn.functional as F \n",
    "import torch \n",
    "from skimage import measure\n",
    "from statistics import mean\n",
    "\n",
    "def compute_pro(masks: np.ndarray, amaps: np.ndarray, num_th: int = 200) -> None:\n",
    "\n",
    "    \"\"\"Compute the area under the curve of per-region overlaping (PRO) and 0 to 0.3 FPR\n",
    "    Args:\n",
    "        category (str): Category of product\n",
    "        masks (ndarray): All binary masks in test. masks.shape -> (num_test_data, h, w)\n",
    "        amaps (ndarray): All anomaly maps in test. amaps.shape -> (num_test_data, h, w)\n",
    "        num_th (int, optional): Number of thresholds\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(amaps, ndarray), \"type(amaps) must be ndarray\"\n",
    "    assert isinstance(masks, ndarray), \"type(masks) must be ndarray\"\n",
    "    assert amaps.ndim == 3, \"amaps.ndim must be 3 (num_test_data, h, w)\"\n",
    "    assert masks.ndim == 3, \"masks.ndim must be 3 (num_test_data, h, w)\"\n",
    "    assert amaps.shape == masks.shape, \"amaps.shape and masks.shape must be same\"\n",
    "    #assert set(masks.flatten()) == {0, 1}, \"set(masks.flatten()) must be {0, 1}\"\n",
    "    assert isinstance(num_th, int), \"type(num_th) must be int\"\n",
    "\n",
    "    df = pd.DataFrame([], columns=[\"pro\", \"fpr\", \"threshold\"])\n",
    "    binary_amaps = np.zeros_like(amaps, dtype=np.bool)\n",
    "\n",
    "    min_th = amaps.min()\n",
    "    max_th = amaps.max()\n",
    "    delta = (max_th - min_th) / num_th\n",
    "\n",
    "    for i,th in enumerate(np.arange(min_th, max_th, delta)):\n",
    "        binary_amaps[amaps <= th] = 0\n",
    "        binary_amaps[amaps > th] = 1\n",
    "\n",
    "        pros = []\n",
    "        for binary_amap, mask in zip(binary_amaps, masks):\n",
    "            for region in measure.regionprops(measure.label(mask)):\n",
    "                axes0_ids = region.coords[:, 0]\n",
    "                axes1_ids = region.coords[:, 1]\n",
    "                tp_pixels = binary_amap[axes0_ids, axes1_ids].sum()\n",
    "                pros.append(tp_pixels / region.area)\n",
    "\n",
    "        inverse_masks = 1 - masks\n",
    "        fp_pixels = np.logical_and(inverse_masks, binary_amaps).sum()\n",
    "        fpr = fp_pixels / inverse_masks.sum()\n",
    "\n",
    "        \n",
    "        df.loc[i,:] = [mean(pros),fpr,th]\n",
    "    # Normalize FPR from 0 ~ 1 to 0 ~ 0.3\n",
    "    df = df[df[\"fpr\"] < 0.3]\n",
    "    df[\"fpr\"] = df[\"fpr\"] / df[\"fpr\"].max()\n",
    "\n",
    "    pro_auc = auc(df[\"fpr\"], df[\"pro\"])\n",
    "    return pro_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:556: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "from train import get_score_map\n",
    "\n",
    "dataloader = testloader \n",
    "true_labels = [] \n",
    "score_list = [] \n",
    "true_gts = []\n",
    "\n",
    "model.eval()\n",
    "for idx, (images, labels, gts) in enumerate(dataloader):\n",
    "    # predict\n",
    "    t_f, s_f = model._forward(images)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, s in zip(t_f, s_f):\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_t_I_k = t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_t_I_k_ij = f_t_I_k[:,0,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
