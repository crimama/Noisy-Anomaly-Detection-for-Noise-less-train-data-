{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tqdm \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn\n",
    "import os\n",
    "os.chdir('/Volume/VAD/UAADF/')\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import  torch.nn.functional as F \n",
    "from arguments import parser\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import create_dataset\n",
    "from accelerate import Accelerator\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from utils import img_show, img_cvt\n",
    "\n",
    "from main import torch_seed\n",
    "import random \n",
    "\n",
    "from refinement.sampler import SubsetSequentialSampler\n",
    "from refinement.refinement import Refinementer\n",
    "\n",
    "\n",
    "torch_seed(0)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' \n",
    "\n",
    "def prepare(default_setting, class_name, anomaly_ratio, baseline, weight_method, threshold):\n",
    "    # dataset = 'pc_mvtecad' if dataset == 'mvtecad' else 'pc_mvtecloco'\n",
    "    # default_setting = f'./configs/benchmark/{dataset}.yaml'\n",
    "    cfg = parser(jupyter=True, default_setting = default_setting)\n",
    "    cfg.DATASET.class_name = class_name \n",
    "    cfg.DATASET.params.anomaly_ratio = anomaly_ratio\n",
    "    cfg.DATASET.params.baseline = baseline \n",
    "    \n",
    "    trainset, testset = create_dataset(\n",
    "        dataset_name  = cfg.DATASET.dataset_name,\n",
    "        datadir       = cfg.DATASET.datadir,\n",
    "        class_name    = cfg.DATASET.class_name,\n",
    "        img_size      = cfg.DATASET.img_size,\n",
    "        mean          = cfg.DATASET.mean,\n",
    "        std           = cfg.DATASET.std,\n",
    "        aug_info      = cfg.DATASET.aug_info,\n",
    "        **cfg.DATASET.get('params',{})\n",
    "    )\n",
    "\n",
    "    method            = cfg.MODEL.method\n",
    "    backbone          = cfg.MODEL.backbone\n",
    "    model_params      = cfg.MODEL.get('params',{})\n",
    "\n",
    "    batch_size       = cfg.DATASET.batch_size\n",
    "    test_batch_size  = cfg.DATASET.test_batch_size\n",
    "    num_workers      = cfg.DATASET.num_workers\n",
    "\n",
    "    # # define train dataloader\n",
    "    trainloader = DataLoader(\n",
    "        dataset     = trainset,\n",
    "        batch_size  = batch_size,\n",
    "        num_workers = num_workers,\n",
    "        shuffle     = False\n",
    "    )\n",
    "\n",
    "    # define test dataloader\n",
    "    testloader = DataLoader(\n",
    "        dataset     = testset,\n",
    "        batch_size  = test_batch_size,\n",
    "        shuffle     = False,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "\n",
    "    refinement = Refinementer(\n",
    "            model          = __import__('models').__dict__[method](\n",
    "                            backbone = backbone,\n",
    "                            **model_params\n",
    "                            ),\n",
    "            n_query        = cfg.REFINEMENT.n_query,\n",
    "            dataset        = trainset,\n",
    "            unrefined_idx  = np.ones(len(trainset)).astype(np.bool8),\n",
    "            batch_size     = batch_size,\n",
    "            test_transform = testset.transform,\n",
    "            num_workers    = num_workers\n",
    "        )\n",
    "    model = refinement.init_model()\n",
    "    device = cfg.MODEL.params.device\n",
    "    \n",
    "    output = {}\n",
    "    output['trainloader'], output['testloader'], output['model'], output['device']  = trainloader, testloader, model, device\n",
    "    \n",
    "    return output \n",
    "\n",
    "def train(inputs):\n",
    "    trainloader, device, model = inputs['trainloader'], inputs['device'], inputs['model']\n",
    "    for imgs, labels, gts in trainloader:\n",
    "        output = model(imgs.to(device))\n",
    "        loss = model.criterion(output)\n",
    "    model.fit()\n",
    "    \n",
    "def evaluation(inputs, loco = False):\n",
    "    testloader,  model = inputs['testloader'], inputs['model']\n",
    "    from utils.metrics import MetricCalculator, loco_auroc\n",
    "\n",
    "    model.eval()\n",
    "    img_level = MetricCalculator(metric_list = ['auroc','average_precision','confusion_matrix'])\n",
    "    pix_level = MetricCalculator(metric_list = ['auroc','average_precision','confusion_matrix','aupro'])\n",
    "\n",
    "    results = {} \n",
    "    for idx, (images, labels, gts) in enumerate(testloader):\n",
    "        \n",
    "        # predict\n",
    "        if model.__class__.__name__ in ['PatchCore']:\n",
    "            score, score_map = model.get_score_map(images)\n",
    "                \n",
    "        # Stack Scoring for metrics \n",
    "        pix_level.update(score_map,gts.type(torch.int))\n",
    "        img_level.update(score, labels.type(torch.int))\n",
    "        \n",
    "    p_results = pix_level.compute()\n",
    "    i_results = img_level.compute()\n",
    "    \n",
    "    \n",
    "    if loco:\n",
    "        results['loco_auroc'] = loco_auroc(pix_level,testloader)\n",
    "        results['loco_auroc'] = loco_auroc(img_level,testloader)    \n",
    "        return p_results, i_results, results \n",
    "    else:         \n",
    "        return p_results, i_results\n",
    "\n",
    "def patch_scoring(testloader, model):\n",
    "    self = model \n",
    "    score_list = [] \n",
    "    for imgs, labels, gts in testloader: \n",
    "        images = imgs.to(torch.float).to(self.device)\n",
    "        _ = self.forward_modules.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features, patch_shapes = self._embed(images, provide_patch_shapes=True)\n",
    "            features = np.asarray(features)\n",
    "\n",
    "            image_scores, _, indices = self.anomaly_scorer.predict([features])\n",
    "        \n",
    "        score_list.append(image_scores)\n",
    "    score_list = np.concatenate(score_list)\n",
    "    return score_list \n",
    "\n",
    "def test_scoring(inputs):\n",
    "    'test 데이터들의 각 anomaly score 산출'\n",
    "    score_list = [] \n",
    "    score_map_list = [] \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels, gts in inputs['testloader']:\n",
    "            score, score_map = inputs['model'].get_score_map(imgs)\n",
    "            score_list.append(score)\n",
    "            score_map_list.append(score_map)\n",
    "    S = np.concatenate(score_list)\n",
    "    SM = np.concatenate(score_map_list)\n",
    "    return S, SM \n",
    "\n",
    "def scaling(inputs):\n",
    "    inputs = (inputs - np.min(inputs)) / (np.max(inputs) - np.min(inputs))\n",
    "    return inputs \n",
    "        \n",
    "        \n",
    "def get_indicies(inputs, lof:bool = False):\n",
    "    '''\n",
    "    denoising 한 index와 coreset index 구하는 메소드 \n",
    "    '''\n",
    "    self = inputs['model']\n",
    "    \n",
    "    train_embeddings = np.vstack([inputs['model'].embed(d.to('cuda')) for d,_,_ in inputs['trainloader']])\n",
    "    features = train_embeddings\n",
    "    \n",
    "    if lof:\n",
    "        with torch.no_grad():\n",
    "            # pdb.set_trace()\n",
    "            self.feature_shape = [28,28]\n",
    "            patch_weight = self._compute_patch_weight(features) # <- get outlier score \n",
    "\n",
    "            # normalization\n",
    "            # patch_weight = (patch_weight - patch_weight.quantile(0.5, dim=1, keepdim=True)).reshape(-1) + 1\n",
    "\n",
    "            patch_weight = patch_weight.reshape(-1)\n",
    "            threshold = torch.quantile(patch_weight, 1 - self.threshold)\n",
    "            sampling_weight = torch.where(patch_weight > threshold, 0, 1) #! sampling_weight = denoising 한 index \n",
    "            #self.featuresampler.set_sampling_weight(sampling_weight) # <- subsampling data which has outlier score under thresholding\n",
    "            #self.patch_weight = patch_weight.clamp(min=0)\n",
    "    \n",
    "    sample_features, sample_indices = self.featuresampler.run(features) #! sample_indices = coreset index         \n",
    "                \n",
    "    if lof:\n",
    "        return {'denoising':sampling_weight.detach().cpu(), 'coreset': sample_indices}\n",
    "    else:\n",
    "        return {'coreset': sample_indices}\n",
    "                \n",
    "# 1024 512 256 128 \n",
    "class DAE(nn.Module):\n",
    "    def __init__(self, in_channels:int, noise_factor:float):\n",
    "        super(DAE, self).__init__()\n",
    "        self.in_c = in_channels \n",
    "        self.encoder = nn.Sequential(*[self.get_linaer_layer(int(self.in_c/(2**i)),int(self.in_c/(2**(i+1)))) for i in range(3)])\n",
    "        self.decoder = nn.Sequential(*[self.get_linaer_layer(int(self.in_c/(2**i)),int(self.in_c/(2**(i-1)))) for i in range(3,0,-1)])\n",
    "        \n",
    "        self.noise_factor = noise_factor\n",
    "        \n",
    "    def get_linaer_layer(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_channels, out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            x = x + self.noise_factor * torch.randn(*x.size()).to(x.device)\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x     \n",
    "    \n",
    "def get_patch_embed(model, trainloader):\n",
    "    for imgs, labels, gts in trainloader:\n",
    "        outputs = model(imgs)\n",
    "    _ = model.forward_modules.eval()\n",
    "    \n",
    "    features = [] \n",
    "    with tqdm.tqdm(model.data, leave=True) as data_iterator:\n",
    "        for image in data_iterator:\n",
    "            with torch.no_grad():\n",
    "                input_image = image.to(torch.float).to(model.device)\n",
    "            feature = model._embed(input_image)\n",
    "            features.append(feature)\n",
    "    features = np.concatenate(features)        \n",
    "    return features    \n",
    "\n",
    "def dae_training(EPOCH, features, criterion, model, scheduler, optimizer,BATCH_SIZE, device):\n",
    "    for e in tqdm.tqdm(range(EPOCH)):\n",
    "        length = len(features)//256\n",
    "        iterator = np.arange(length)\n",
    "        np.random.shuffle(iterator)\n",
    "        \n",
    "        losses = [] \n",
    "        for i in range(length):\n",
    "            data = features[i:i+BATCH_SIZE,:]\n",
    "            data = torch.Tensor(data).to(device)\n",
    "            \n",
    "            # predict\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data)\n",
    "            loss.backward()\n",
    "            losses.append(loss.detach().cpu().numpy())\n",
    "            \n",
    "            # loss update \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        \n",
    "        scheduler.step()\n",
    "        print(np.mean(losses))\n",
    "        \n",
    "    recon_x = [] \n",
    "    with torch.no_grad():\n",
    "        for i in range(length):\n",
    "            data = features[i:i+BATCH_SIZE,:]\n",
    "            data = torch.Tensor(data).to(device)\n",
    "            \n",
    "            # predict\n",
    "            output = model(data)\n",
    "            \n",
    "            recon_x.append(output.detach().cpu().numpy())\n",
    "    recon_x = np.concatenate(recon_x)\n",
    "    return recon_x \n",
    "\n",
    "'''\n",
    "test 이미지도 dae inference 한 것 적용하기 위한 코드 \n",
    "'''\n",
    "from utils.metrics import MetricCalculator\n",
    "def dae_direct_evaluation(model, dae, testloader,device):\n",
    "    model.eval()\n",
    "    dae.eval()\n",
    "    img_level = MetricCalculator(metric_list = ['auroc','average_precision','confusion_matrix'])\n",
    "    pix_level = MetricCalculator(metric_list = ['auroc','average_precision','confusion_matrix','aupro'])\n",
    "\n",
    "    self = model \n",
    "    results = {} \n",
    "    for idx, (images, labels, gts) in enumerate(testloader):\n",
    "        images = images.to(torch.float).to(device)\n",
    "        _ = self.forward_modules.eval()\n",
    "        \n",
    "        batchsize = images.shape[0]\n",
    "        with torch.no_grad():\n",
    "            features, patch_shapes = self._embed(images, provide_patch_shapes=True)\n",
    "            features = np.asarray(features)\n",
    "            ###\n",
    "            features = dae.decoder(dae.encoder(torch.Tensor(features).to('cuda'))).detach().cpu().numpy()\n",
    "            ###\n",
    "        image_scores, _, indices = self.anomaly_scorer.predict([features]) \n",
    "        \n",
    "        patch_scores = image_scores\n",
    "\n",
    "        image_scores = self.patch_maker.unpatch_scores(\n",
    "            image_scores, batchsize=batchsize\n",
    "        )\n",
    "        image_scores = image_scores.reshape(*image_scores.shape[:2], -1)\n",
    "        image_scores = self.patch_maker.score(image_scores)\n",
    "\n",
    "        patch_scores = self.patch_maker.unpatch_scores(\n",
    "            patch_scores, batchsize=batchsize\n",
    "        ) # Unfold : (B)\n",
    "        scales = patch_shapes[0]\n",
    "        patch_scores = patch_scores.reshape(batchsize, scales[0], scales[1])\n",
    "\n",
    "        masks = self.anomaly_segmentor.convert_to_segmentation(patch_scores) # interpolation : (B,pw,ph) -> (B,W,H)\n",
    "\n",
    "        score = [score for score in image_scores]\n",
    "        score_map = [mask for mask in masks]       \n",
    "        \n",
    "        score = np.array(score)\n",
    "        score_map = np.concatenate([np.expand_dims(sm,0) for sm in score_map]) #(B,W,H)\n",
    "        score_map = np.expand_dims(score_map,1) # (B,1,W,H)\n",
    "        \n",
    "        pix_level.update(score_map,gts.type(torch.int))\n",
    "        img_level.update(score, labels.type(torch.int))\n",
    "        \n",
    "    p_results = pix_level.compute()\n",
    "    i_results = img_level.compute()\n",
    "                \n",
    "    return p_results['auroc'], i_results['auroc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arguments import parser\n",
    "default_setting = 'configs/default/mvtecad.yaml'\n",
    "model_setting = 'configs/model/patchcore.yaml'\n",
    "\n",
    "cfg = parser(True,default_setting, model_setting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'datadir': '../Data', 'batch_size': 32, 'test_batch_size': 32, 'num_workers': 8, 'dataset_name': 'MVTecAD', 'aug_info': ['Resize'], 'class_name': 'toothbrush', 'img_size': 224, 'params': {'anomaly_ratio': 0.0, 'baseline': False}, 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['DATASET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n"
     ]
    }
   ],
   "source": [
    "exec(f\"{model.lower()}_arguments('a')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'mvtecad'\n",
    "CLASSNAME = 'screw'\n",
    "ANOMALYRATIO = 0\n",
    "\n",
    "EPOCH = 30\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "RESULTS = [] \n",
    "for CLASSNAME in ['cable','bottle','carpet','grid','leather','metal_nut','pill','screw','tile','toothbrush','wood','zipper']:\n",
    "    for ANOMALYRATIO in [0.2]:\n",
    "        inputs = prepare('mvtecad',CLASSNAME,ANOMALYRATIO,False,'lof',0.15)\n",
    "        model = inputs['model']\n",
    "        trainloader = inputs['trainloader']\n",
    "        testloader = inputs['testloader']\n",
    "        features = get_patch_embed(model, trainloader)\n",
    "\n",
    "        dae = DAE(1024,0.01).to(model.device)\n",
    "\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(dae.parameters(), lr=LR)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = EPOCH, eta_min = 0.00001)\n",
    "\n",
    "        recon_x = dae_training(EPOCH, features, criterion, dae, scheduler, optimizer, BATCH_SIZE, 'cuda')\n",
    "\n",
    "        #sampling weight \n",
    "        #!score = np.mean((features - recon_x)**2,axis=1)**0.5\n",
    "        #!sampling_weight = np.where(score > np.percentile(score,0.85),0,1)\n",
    "        #!model.featuresampler.set_sampling_weight(sampling_weight)\n",
    "\n",
    "        sample_features, sample_indices = model.featuresampler.run(recon_x) # greedy search\n",
    "        model.anomaly_scorer.fit(detection_features=[sample_features])\n",
    "        p_auroc, i_auroc = dae_direct_evaluation(model, dae, testloader, model.device)\n",
    "        \n",
    "        RESULTS.append([CLASSNAME, ANOMALYRATIO, p_auroc, i_auroc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ['cable', ANOMALYRATIO, p_auroc, i_auroc]\n",
    "with open('./results/MVTecAD/dae/log.txt', 'a') as f: \n",
    "                f.write(str(result)+'\\n')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
