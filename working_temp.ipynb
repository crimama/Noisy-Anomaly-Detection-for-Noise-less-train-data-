{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn\n",
    "import os\n",
    "os.chdir('/Volume/VAD/UAADF/')\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import  torch.nn.functional as F \n",
    "from arguments import parser\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import create_dataset\n",
    "from accelerate import Accelerator\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from utils import img_show, img_cvt\n",
    "\n",
    "from main import torch_seed\n",
    "import random \n",
    "\n",
    "from query_strategies.sampler import SubsetSequentialSampler\n",
    "from query_strategies.refinement import Refinementer\n",
    "\n",
    "\n",
    "torch_seed(0)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' \n",
    "\n",
    "def prepare(dataset, class_name, anomaly_ratio, baseline, weight_method, threshold):\n",
    "    dataset = 'pc_mvtecad' if dataset == 'mvtecad' else 'pc_mvtecloco'\n",
    "    default_setting = f'./configs/benchmark/{dataset}.yaml'\n",
    "    cfg = parser(jupyter=True, default_setting = default_setting)\n",
    "    cfg.DATASET.class_name = class_name \n",
    "    cfg.DATASET.params.anomaly_ratio = anomaly_ratio\n",
    "    cfg.DATASET.params.baseline = baseline \n",
    "    cfg.MODEL.params.weight_method = weight_method \n",
    "    cfg.MODEL.params.threshold = threshold\n",
    "    \n",
    "    trainset, testset = create_dataset(\n",
    "        dataset_name  = cfg.DATASET.dataset_name,\n",
    "        datadir       = cfg.DATASET.datadir,\n",
    "        class_name    = cfg.DATASET.class_name,\n",
    "        img_size      = cfg.DATASET.img_size,\n",
    "        mean          = cfg.DATASET.mean,\n",
    "        std           = cfg.DATASET.std,\n",
    "        aug_info      = cfg.DATASET.aug_info,\n",
    "        **cfg.DATASET.get('params',{})\n",
    "    )\n",
    "\n",
    "    method            = cfg.MODEL.method\n",
    "    backbone          = cfg.MODEL.backbone\n",
    "    model_params      = cfg.MODEL.get('params',{})\n",
    "\n",
    "    batch_size       = cfg.DATASET.batch_size\n",
    "    test_batch_size  = cfg.DATASET.test_batch_size\n",
    "    num_workers      = cfg.DATASET.num_workers\n",
    "\n",
    "    # # define train dataloader\n",
    "    trainloader = DataLoader(\n",
    "        dataset     = trainset,\n",
    "        batch_size  = batch_size,\n",
    "        num_workers = num_workers,\n",
    "        shuffle     = False\n",
    "    )\n",
    "\n",
    "    # define test dataloader\n",
    "    testloader = DataLoader(\n",
    "        dataset     = testset,\n",
    "        batch_size  = test_batch_size,\n",
    "        shuffle     = False,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "\n",
    "    refinement = Refinementer(\n",
    "            model          = __import__('models').__dict__[method](\n",
    "                            backbone = backbone,\n",
    "                            **model_params\n",
    "                            ),\n",
    "            n_query        = cfg.REFINEMENT.n_query,\n",
    "            dataset        = trainset,\n",
    "            unrefined_idx  = np.ones(len(trainset)).astype(np.bool8),\n",
    "            batch_size     = batch_size,\n",
    "            test_transform = testset.transform,\n",
    "            num_workers    = num_workers\n",
    "        )\n",
    "    model = refinement.init_model()\n",
    "    device = cfg.MODEL.params.device\n",
    "    \n",
    "    output = {}\n",
    "    output['trainloader'], output['testloader'], output['model'], output['device']  = trainloader, testloader, model, device\n",
    "    \n",
    "    return output \n",
    "\n",
    "def train(inputs):\n",
    "    trainloader, device, model = inputs['trainloader'], inputs['device'], inputs['model']\n",
    "    for imgs, labels, gts in trainloader:\n",
    "        output = model(imgs.to(device))\n",
    "        loss = model.criterion(output)\n",
    "    model.fit()\n",
    "    \n",
    "def evaluation(inputs, loco = False):\n",
    "    testloader,  model = inputs['testloader'], inputs['model']\n",
    "    from utils.metrics import MetricCalculator, loco_auroc\n",
    "\n",
    "    model.eval()\n",
    "    img_level = MetricCalculator(metric_list = ['auroc','average_precision','confusion_matrix'])\n",
    "    pix_level = MetricCalculator(metric_list = ['auroc','average_precision','confusion_matrix','aupro'])\n",
    "\n",
    "    results = {} \n",
    "    for idx, (images, labels, gts) in enumerate(testloader):\n",
    "        \n",
    "        # predict\n",
    "        if model.__class__.__name__ in ['PatchCore']:\n",
    "            score, score_map = model.get_score_map(images)\n",
    "                \n",
    "        # Stack Scoring for metrics \n",
    "        pix_level.update(score_map,gts.type(torch.int))\n",
    "        img_level.update(score, labels.type(torch.int))\n",
    "        \n",
    "    p_results = pix_level.compute()\n",
    "    i_results = img_level.compute()\n",
    "    \n",
    "    \n",
    "    if loco:\n",
    "        results['loco_auroc'] = loco_auroc(pix_level,testloader)\n",
    "        results['loco_auroc'] = loco_auroc(img_level,testloader)    \n",
    "        return p_results, i_results, results \n",
    "    else:         \n",
    "        return p_results, i_results\n",
    "\n",
    "def patch_scoring(testloader, model):\n",
    "    self = model \n",
    "    score_list = [] \n",
    "    for imgs, labels, gts in testloader: \n",
    "        images = imgs.to(torch.float).to(self.device)\n",
    "        _ = self.forward_modules.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features, patch_shapes = self._embed(images, provide_patch_shapes=True)\n",
    "            features = np.asarray(features)\n",
    "\n",
    "            image_scores, _, indices = self.anomaly_scorer.predict([features])\n",
    "        \n",
    "        score_list.append(image_scores)\n",
    "    score_list = np.concatenate(score_list)\n",
    "    return score_list \n",
    "\n",
    "def test_scoring(inputs):\n",
    "    'test 데이터들의 각 anomaly score 산출'\n",
    "    score_list = [] \n",
    "    score_map_list = [] \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels, gts in inputs['testloader']:\n",
    "            score, score_map = inputs['model'].get_score_map(imgs)\n",
    "            score_list.append(score)\n",
    "            score_map_list.append(score_map)\n",
    "    S = np.concatenate(score_list)\n",
    "    SM = np.concatenate(score_map_list)\n",
    "    return S, SM \n",
    "\n",
    "def scaling(inputs):\n",
    "    inputs = (inputs - np.min(inputs)) / (np.max(inputs) - np.min(inputs))\n",
    "    return inputs \n",
    "        \n",
    "        \n",
    "def get_indicies(inputs, lof:bool = False):\n",
    "    '''\n",
    "    denoising 한 index와 coreset index 구하는 메소드 \n",
    "    '''\n",
    "    self = inputs['model']\n",
    "    \n",
    "    train_embeddings = np.vstack([inputs['model'].embed(d.to('cuda')) for d,_,_ in inputs['trainloader']])\n",
    "    features = train_embeddings\n",
    "    \n",
    "    if lof:\n",
    "        with torch.no_grad():\n",
    "            # pdb.set_trace()\n",
    "            self.feature_shape = [28,28]\n",
    "            patch_weight = self._compute_patch_weight(features) # <- get outlier score \n",
    "\n",
    "            # normalization\n",
    "            # patch_weight = (patch_weight - patch_weight.quantile(0.5, dim=1, keepdim=True)).reshape(-1) + 1\n",
    "\n",
    "            patch_weight = patch_weight.reshape(-1)\n",
    "            threshold = torch.quantile(patch_weight, 1 - self.threshold)\n",
    "            sampling_weight = torch.where(patch_weight > threshold, 0, 1) #! sampling_weight = denoising 한 index \n",
    "            #self.featuresampler.set_sampling_weight(sampling_weight) # <- subsampling data which has outlier score under thresholding\n",
    "            #self.patch_weight = patch_weight.clamp(min=0)\n",
    "    \n",
    "    sample_features, sample_indices = self.featuresampler.run(features) #! sample_indices = coreset index         \n",
    "                \n",
    "    if lof:\n",
    "        return {'denoising':sampling_weight.detach().cpu(), 'coreset': sample_indices}\n",
    "    else:\n",
    "        return {'coreset': sample_indices}\n",
    "                \n",
    "# 1024 512 256 128 \n",
    "class DAE(nn.Module):\n",
    "    def __init__(self, in_channels:int, noise_factor:float):\n",
    "        super(DAE, self).__init__()\n",
    "        self.in_c = in_channels \n",
    "        self.encoder = nn.Sequential(*[self.get_linaer_layer(int(self.in_c/(2**i)),int(self.in_c/(2**(i+1)))) for i in range(3)])\n",
    "        self.decoder = nn.Sequential(*[self.get_linaer_layer(int(self.in_c/(2**i)),int(self.in_c/(2**(i-1)))) for i in range(3,0,-1)])\n",
    "        \n",
    "        self.noise_factor = noise_factor\n",
    "        \n",
    "    def get_linaer_layer(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_channels, out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x + self.noise_factor * torch.randn(*x.size()).to(x.device)\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x     \n",
    "    \n",
    "def get_patch_embed(model, trainloader):\n",
    "    for imgs, labels, gts in trainloader:\n",
    "        outputs = model(imgs)\n",
    "    _ = model.forward_modules.eval()\n",
    "    \n",
    "    features = [] \n",
    "    with tqdm.tqdm(model.data, leave=True) as data_iterator:\n",
    "        for image in data_iterator:\n",
    "            with torch.no_grad():\n",
    "                input_image = image.to(torch.float).to(model.device)\n",
    "            feature = model._embed(input_image)\n",
    "            features.append(feature)\n",
    "    features = np.concatenate(features)        \n",
    "    return features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.65it/s]\n"
     ]
    }
   ],
   "source": [
    "inputs = prepare('mvtecad','screw',0.2,False,'lof',0.15)\n",
    "model = inputs['model']\n",
    "trainloader = inputs['trainloader']\n",
    "features = get_patch_embed(model, trainloader)\n",
    "\n",
    "dae = DAE(1024,0.01).to(model.device)\n",
    "EPOCH = 30\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(dae.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = EPOCH, eta_min = 0.00001)\n",
    "\n",
    "min = features.min()\n",
    "max = features.max()\n",
    "in_f = (features - min) / (max-min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:11<05:44, 11.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3716006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:23<05:31, 11.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015399909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:36<05:29, 12.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00016266183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:48<05:18, 12.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.844065e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [01:00<05:02, 12.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.742006e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [01:13<04:56, 12.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1931584e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [01:25<04:39, 12.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1721505e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [01:37<04:25, 12.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5109144e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [01:49<04:18, 12.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0807214e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [02:02<04:06, 12.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7946488e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [02:14<03:54, 12.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6240137e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [02:26<03:42, 12.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5178176e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [02:38<03:26, 12.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.44368205e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [02:51<03:15, 12.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3962136e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [03:02<03:01, 12.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3689037e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [03:14<02:48, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3533878e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [03:26<02:36, 12.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3610079e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [03:38<02:23, 11.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.390737e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [03:50<02:11, 11.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.448838e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [04:02<02:00, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5499574e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [04:14<01:47, 11.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.727357e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [04:26<01:34, 11.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0250405e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [04:38<01:23, 11.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5221554e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [04:49<01:10, 11.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3578173e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [05:01<00:58, 11.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7021953e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [05:13<00:47, 11.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.563705e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [05:25<00:35, 11.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.901344e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [05:37<00:23, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00011384854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [05:49<00:11, 11.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00013396298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [06:01<00:00, 12.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00014428156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for e in tqdm.tqdm(range(EPOCH)):\n",
    "    length = len(features)//256\n",
    "    iterator = np.arange(length)\n",
    "    np.random.shuffle(iterator)\n",
    "    \n",
    "    losses = [] \n",
    "    for i in range(length):\n",
    "        data = in_f[i:i+BATCH_SIZE,:]\n",
    "        data = torch.Tensor(data).to(model.device)\n",
    "        \n",
    "        # predict\n",
    "        output = dae(data)\n",
    "        loss = criterion(output, data)\n",
    "        loss.backward()\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        \n",
    "        # loss update \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    \n",
    "    scheduler.step()\n",
    "    print(np.mean(losses))\n",
    "    \n",
    "recon_x = [] \n",
    "with torch.no_grad():\n",
    "    for i in range(length):\n",
    "        data = features[i:i+BATCH_SIZE,:]\n",
    "        data = torch.Tensor(data).to(model.device)\n",
    "        \n",
    "        # predict\n",
    "        output = dae(data)\n",
    "        \n",
    "        recon_x.append(output.detach().cpu().numpy())\n",
    "recon_x = np.concatenate(recon_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     model.feature_shape = model._embed(model.data[0][:1].to(torch.float).to(model.device), provide_patch_shapes=True)[1][0]\n",
    "     patch_weight = model._compute_patch_weight(features)\n",
    "     \n",
    "     patch_weight = patch_weight.reshape(-1)\n",
    "     threshold = torch.quantile(patch_weight, 1 - model.threshold)\n",
    "     sampling_weight = torch.where(patch_weight > threshold, 0, 1) \n",
    "model.featuresampler.set_sampling_weight(sampling_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subsampling...: 100%|██████████| 25088/25088 [00:18<00:00, 1361.79it/s]\n"
     ]
    }
   ],
   "source": [
    "sample_features, sample_indices = model.featuresampler.run(recon_x * (max-min) + min ) # greedy search\n",
    "model.anomaly_scorer.fit(detection_features=[sample_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8889174126852853 0.4390243902439024\n"
     ]
    }
   ],
   "source": [
    "inputs['model'] = model \n",
    "p_results, i_results = evaluation(inputs)\n",
    "print(p_results['auroc'], i_results['auroc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lof_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "lof = LocalOutlierFactor(6,metric='l2')\n",
    "lof.fit(features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
