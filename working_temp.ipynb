{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FeatureExtractor is deprecated. Use TimmFeatureExtractor instead. Both FeatureExtractor and TimmFeatureExtractor will be removed in a future release.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from arguments import parser\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import create_dataset\n",
    "from accelerate import Accelerator\n",
    "import matplotlib.pyplot as plt \n",
    "from utils.utils import img_show, img_cvt\n",
    "\n",
    "\n",
    "import random \n",
    "def torch_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed) # if use multi-GPU \n",
    "    # CUDA randomness\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "    \n",
    "torch_seed(42)\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "default_setting = './configs/benchmark/rd_cifar10.yaml'\n",
    "cfg = parser(jupyter=True, default_setting = default_setting)\n",
    "\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision             = cfg.TRAIN.mixed_precision\n",
    ")\n",
    "\n",
    "# load dataset\n",
    "trainset, testset = create_dataset(\n",
    "    dataset_name  = cfg.DATASET.dataset_name,\n",
    "    datadir       = cfg.DATASET.datadir,\n",
    "    class_name    = cfg.DATASET.class_name,\n",
    "    img_size      = cfg.DATASET.img_size,\n",
    "    mean          = cfg.DATASET.mean,\n",
    "    std           = cfg.DATASET.std,\n",
    "    aug_info      = cfg.DATASET.aug_info,\n",
    "    **cfg.DATASET.get('params',{})\n",
    ")\n",
    "\n",
    "# make save directory\n",
    "savedir = os.path.join(\n",
    "                            cfg.DEFAULT.savedir,\n",
    "                            cfg.DATASET.dataset_name,\n",
    "                            cfg.DATASET.class_name\n",
    "                        )\n",
    "\n",
    "exp_name         = cfg.DEFAULT.exp_name\n",
    "method            = cfg.MODEL.method\n",
    "backbone          = cfg.MODEL.backbone\n",
    "model_params      = cfg.MODEL.get('params',{})\n",
    "\n",
    "batch_size       = cfg.DATASET.batch_size\n",
    "test_batch_size  = cfg.DATASET.test_batch_size\n",
    "num_workers      = cfg.DATASET.num_workers\n",
    "\n",
    "opt_name         = cfg.OPTIMIZER.opt_name\n",
    "lr               = cfg.OPTIMIZER.lr\n",
    "opt_params       = cfg.OPTIMIZER.get('params',{})\n",
    "\n",
    "epochs           = cfg.TRAIN.epochs\n",
    "log_interval     = cfg.TRAIN.log_interval\n",
    "use_wandb        = cfg.TRAIN.wandb.use\n",
    "\n",
    "savedir          = savedir\n",
    "seed             = cfg.DEFAULT.seed\n",
    "accelerator      = accelerator\n",
    "cfg              = cfg\n",
    "\n",
    "# # define train dataloader\n",
    "trainloader = DataLoader(\n",
    "    dataset     = trainset,\n",
    "    batch_size  = batch_size,\n",
    "    num_workers = num_workers,\n",
    "    shuffle     = True \n",
    ")\n",
    "\n",
    "# define test dataloader\n",
    "testloader = DataLoader(\n",
    "    dataset     = testset,\n",
    "    batch_size  = test_batch_size,\n",
    "    shuffle     = False,\n",
    "    num_workers = num_workers\n",
    ")\n",
    "\n",
    "model = __import__('models').__dict__[method](\n",
    "        backbone = backbone,\n",
    "        **model_params\n",
    "        )\n",
    "\n",
    "# #optimizer = __import__('torch.optim', fromlist='optim').__dict__[opt_name](model.parameters(), lr=lr, **opt_params)\n",
    "# model.load_state_dict(torch.load('results/MVTecAD/ReverseDistillation/bottle/imagenet_stats-anomaly_ratio_0/seed_42/model_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_loss = torch.nn.CosineSimilarity()\n",
    "\n",
    "imgs,labels,gts = next(iter(trainloader))\n",
    "\n",
    "output = model(imgs)\n",
    "e_output, d_output = output\n",
    "\n",
    "e1,e2,e3 = e_output \n",
    "d1,d2,d3 = d_output\n",
    "\n",
    "loss_sum = 0 \n",
    "for e,d in zip([e1,e2,e3],[d1,d2,d3]):\n",
    "    loss_sum += torch.mean(\n",
    "                    1\n",
    "                    - cos_loss(\n",
    "                        e.view(e.shape[0], -1),\n",
    "                        d.view(d.shape[0], -1),\n",
    "                    )\n",
    "                )\n",
    "grad = torch.autograd.grad(loss_sum, (d1,d2,d3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "emb = [] \n",
    "for i,g in enumerate(grad):\n",
    "    g = g.mean(1)\n",
    "    g_norm = torch.norm(g,dim=(1,2))\n",
    "    emb.append(g_norm.unsqueeze(1))\n",
    "embs = torch.hstack(emb)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = torch.argmax(torch.norm(embs, 2,1)).item()\n",
    "mu = [embs[ind]]\n",
    "\n",
    "D2 = torch.cdist(mu[-1].view(1,-1), embs, 2)[0].cpu().numpy()\n",
    "\n",
    "D2 = D2.ravel().astype(float)\n",
    "Ddist = (D2 ** 2)/ sum(D2 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "indsAll = init_centers(X = embs, K = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 8, 6, 11, 7, 13, 14, 4, 9, 5]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indsAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "@torch.no_grad()\n",
    "def init_centers(X: torch.Tensor, K: int = None):    \n",
    "    '''\n",
    "    K-MEANS++ seeding algorithm \n",
    "    - 초기 센터는 gradient embedding의 norm이 가장 큰 것으로 사용 \n",
    "    - 그 이후 부터는 앞서 선택 된 center와의 거리를 계산, 이와 비례하는 확률로 이산 확률 분포를 만듬\n",
    "    - 이 이산 확률 분포에서 새로운 센터를 선택 \n",
    "    - 이렇게 뽑힌 센터들은 k-means 목적함수의 기대값에 근사되는 것이 증명 됨, 따라서 다양성을 확보할 수 있음 (Arthur and Vassilvitskii, 2007)\n",
    "    '''    \n",
    "    # K-means ++ initializing\n",
    "    embs = torch.Tensor(X)\n",
    "    ind = torch.argmax(torch.norm(embs, 2, 1)).item()\n",
    "    embs = embs.cuda()\n",
    "    \n",
    "    mu = [embs[ind]]\n",
    "    indsAll = [ind]\n",
    "    centInds = [0.] * len(embs)\n",
    "    cent = 0\n",
    "    \n",
    "    # Sampling \n",
    "    while len(mu) < K:\n",
    "        if len(mu) == 1:\n",
    "            D2 = torch.cdist(mu[-1].view(1,-1), embs, 2)[0].cpu().numpy() # Calculate l2 Distance btw mu and embs \n",
    "        else:\n",
    "            newD = torch.cdist(mu[-1].view(1,-1), embs, 2)[0].cpu().numpy() # Calculate l2 Distance btw mu and embs \n",
    "            for i in range(len(embs)):\n",
    "                if D2[i] >  newD[i]:\n",
    "                    centInds[i] = cent\n",
    "                    D2[i] = newD[i]            \n",
    "        D2 = D2.ravel().astype(float)\n",
    "        \n",
    "        Ddist = (D2 ** 2)/ sum(D2 ** 2) \n",
    "        customDist = stats.rv_discrete(name='custm', values=(np.arange(len(D2)), Ddist)) # 이산 확률 분포 구축 \n",
    "        ind = customDist.rvs(size=1)[0] # 이산 확률 분포에서 하나 추출 \n",
    "        \n",
    "        while ind in indsAll: ind = customDist.rvs(size=1)[0] # repeat until choosing index not in indsAll \n",
    "        \n",
    "        mu.append(embs[ind])\n",
    "        indsAll.append(ind)\n",
    "        cent += 1\n",
    "    return indsAll"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
